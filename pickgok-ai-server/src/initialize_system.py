import os
import sys
import json
import time
import logging
import pandas as pd
import numpy as np
import librosa
import faiss
import mysql.connector
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Tuple, Optional


# --- Configuration ---
class Config:
    # 1. í˜„ì¬ íŒŒì¼(src)ì˜ ìœ„ì¹˜ ê¸°ì¤€
    CURRENT_FILE = Path(__file__).resolve()
    AI_SERVER_ROOT = CURRENT_FILE.parent.parent  # pickgok-ai-server
    PROJECT_ROOT = AI_SERVER_ROOT.parent         # PickGok-Project (Root)

    # 2. ë°ì´í„° ê²½ë¡œ (ì›¹ ì„œë²„ì˜ data í´ë” ì°¸ì¡°)
    WEB_DATA_DIR = PROJECT_ROOT / "pickgok-web-server" / "src" / "main" / "webapp" / "data"

    # 3. ì„¸ë¶€ ê²½ë¡œ
    AUDIO_DIR = WEB_DATA_DIR / "fma_small"
    METADATA_CSV_PATH = WEB_DATA_DIR / "fma_metadata" / "tracks.csv"

    # 4. ëª¨ë¸ ì¶œë ¥ ê²½ë¡œ
    OUTPUT_DIR = AI_SERVER_ROOT / "models"

    # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
    SAMPLE_RATE = 22050
    DURATION = 29.0
    N_MFCC = 20

    # ì‹œìŠ¤í…œ ì„¤ì • (CPU ì½”ì–´ ì ˆë°˜ ì‚¬ìš©)
    MAX_WORKERS = max(1, (os.cpu_count() or 2) // 2)

    # MySQL ì ‘ì† ì •ë³´
    DB_CONFIG = {
        "host": "localhost",
        "user": "root",
        "password": "0000",  # [ì¤‘ìš”] ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸ í™•ì¸
        "database": "pick_gok",
        "auth_plugin": "mysql_native_password",
    }

    # ë¡œê¹… ì„¤ì •
    LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"


# ì´ˆê¸°í™”
logging.basicConfig(level=logging.INFO, format=Config.LOG_FORMAT)
logger = logging.getLogger("InitSystem")
os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

# =========================================================
# [Common] ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹°
# =========================================================


def load_tracks_metadata() -> pd.DataFrame:
    """tracks.csv ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    if not Config.METADATA_CSV_PATH.exists():
        logger.error(f"âŒ File not found: {Config.METADATA_CSV_PATH}")
        sys.exit(1)

    logger.info(f"Loading metadata from: {Config.METADATA_CSV_PATH}")
    try:
        tracks = pd.read_csv(Config.METADATA_CSV_PATH, index_col=0, header=[0, 1])
        small_subset = tracks[tracks[("set", "subset")] == "small"]

        df_clean = pd.DataFrame(
            {
                "track_id": small_subset.index,
                "title": small_subset[("track", "title")],
                "artist": small_subset[("artist", "name")],
                "genre": small_subset[("track", "genre_top")],
                "duration": small_subset[("track", "duration")],
            }
        ).set_index("track_id")

        # NaN ê°’ ì²˜ë¦¬
        df_clean["genre"] = df_clean["genre"].fillna("Unknown")

        logger.info(f"âœ… Metadata loaded. Total tracks: {len(df_clean)}")
        return df_clean

    except Exception as e:
        logger.error(f"Failed to load metadata: {e}")
        sys.exit(1)


def get_audio_path(track_id: int) -> Path:
    """ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œ ê²½ë¡œ (Librosaìš©)"""
    tid_str = "{:06d}".format(track_id)
    folder_code = tid_str[:3]
    return Config.AUDIO_DIR / folder_code / f"{tid_str}.mp3"


def get_web_audio_path(track_id: int) -> str:
    """Java ì›¹ ì„œë²„ìš© ìƒëŒ€ ê²½ë¡œ (DB ì €ì¥ìš©)"""
    tid_str = "{:06d}".format(track_id)
    folder_code = tid_str[:3]
    # ì›¹ ì„œë²„ì—ì„œëŠ” webapp/data í´ë”ê°€ /data URLë¡œ ë§¤í•‘ëœë‹¤ê³  ê°€ì •
    return f"/data/fma_small/{folder_code}/{tid_str}.mp3"


# =========================================================
# [Phase 1] AI ëª¨ë¸ ë¹Œë“œ
# =========================================================


def extract_features(track_id: int) -> Optional[Tuple[int, np.ndarray]]:
    file_path = get_audio_path(track_id)
    if not file_path.exists():
        return None

    try:
        y, sr = librosa.load(file_path, sr=Config.SAMPLE_RATE, duration=Config.DURATION)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=Config.N_MFCC)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_var = np.var(mfcc, axis=1)
        feature_vector = np.concatenate((mfcc_mean, mfcc_var))
        return track_id, feature_vector.astype("float32")
    except Exception:
        return None


def build_ai_model(df: pd.DataFrame):
    logger.info(
        f">>> [Phase 1] Starting AI Model Build (Workers: {Config.MAX_WORKERS})"
    )
    logger.info(f"    - Output Dir: {Config.OUTPUT_DIR}")
    start_time = time.time()

    track_ids = df.index.tolist()
    features = []
    valid_ids = []

    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŠ¹ì§• ì¶”ì¶œ
    with ProcessPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
        future_to_id = {
            executor.submit(extract_features, tid): tid for tid in track_ids
        }

        for future in tqdm(
                as_completed(future_to_id),
                total=len(track_ids),
                desc="AI: Extracting Features",
        ):
            result = future.result()
            if result is not None:
                tid, vec = result
                valid_ids.append(tid)
                features.append(vec)

    if not features:
        logger.error("âŒ No features extracted. AI Model build failed.")
        return

    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    d = features[0].shape[0]
    feature_matrix = np.vstack(features)
    index = faiss.IndexFlatL2(d)
    index.add(feature_matrix)

    # ëª¨ë¸ ì €ì¥
    faiss.write_index(index, str(Config.OUTPUT_DIR / "music.index"))

    # ë©”íƒ€ë°ì´í„° JSON ì €ì¥ (Track ID ë§¤í•‘ìš©)
    mapping = []
    for i, original_id in enumerate(valid_ids):
        try:
            info = df.loc[original_id]
            mapping.append(
                {
                    "faiss_id": i,
                    "track_id": int(original_id),
                    "title": str(info["title"]),
                    "artist": str(info["artist"]),
                }
            )
        except KeyError:
            continue

    with open(Config.OUTPUT_DIR / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    logger.info(f"âœ… [Phase 1] AI Model Built & Saved in {elapsed:.2f}s")


# =========================================================
# [Phase 2] DB ë§ˆì´ê·¸ë ˆì´ì…˜
# =========================================================


def test_db_connection() -> bool:
    logger.info("Checking Database Connection...")
    conn = None
    try:
        conn = mysql.connector.connect(**Config.DB_CONFIG)
        logger.info("âœ… Database connected successfully.")
        return True
    except mysql.connector.Error as err:
        logger.error(f"âŒ DB Connection Failed: {err}")
        return False
    finally:
        if conn:
            conn.close()


def migrate_database(df: pd.DataFrame):
    logger.info(">>> [Phase 2] Starting Database Migration")
    conn = None
    try:
        conn = mysql.connector.connect(**Config.DB_CONFIG)
        cursor = conn.cursor()

        query = """
                INSERT INTO tracks (track_id, title, artist, genre, duration, file_path)
                VALUES (%s, %s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                                         title = VALUES(title),
                                         artist = VALUES(artist),
                                         genre = VALUES(genre),
                                         file_path = VALUES(file_path) \
                """

        data_batch = []
        batch_size = 1000

        for track_id, row in tqdm(
                df.iterrows(), total=len(df), desc="DB: Migrating Data"
        ):
            title = str(row["title"])[:255]
            artist = str(row["artist"])[:255]
            genre = str(row["genre"])
            duration = int(float(row["duration"]))
            # [í•µì‹¬] ì›¹ ì˜¤ë””ì˜¤ ê²½ë¡œ ì‚¬ìš© (/data/...)
            file_path = get_web_audio_path(int(track_id))

            data_batch.append(
                (int(track_id), title, artist, genre, duration, file_path)
            )

            if len(data_batch) >= batch_size:
                cursor.executemany(query, data_batch)
                conn.commit()
                data_batch = []

        if data_batch:
            cursor.executemany(query, data_batch)
            conn.commit()

        logger.info("âœ… [Phase 2] Database Migration Complete.")

    except mysql.connector.Error as err:
        logger.error(f"âŒ Database Migration Failed: {err}")
    finally:
        if conn:
            conn.close()


# =========================================================
# Main Execution Flow
# =========================================================


def main():
    print("\n" + "=" * 50)
    print("ğŸµ PickGok System Initialization")
    print("=" * 50 + "\n")

    # 1. DB ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_db_connection():
        return

    # 2. ê²½ë¡œ í™•ì¸ ë¡œê·¸
    logger.info(f"Project Root: {Config.PROJECT_ROOT}")
    logger.info(f"Web Data Dir: {Config.WEB_DATA_DIR}")
    logger.info(f"Audio Dir   : {Config.AUDIO_DIR}")
    logger.info(f"Models Dir  : {Config.OUTPUT_DIR}")

    # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ
    df = load_tracks_metadata()

    # 4. AI ëª¨ë¸ ë¹Œë“œ (Phase 1)
    build_ai_model(df)

    # 5. DB ë§ˆì´ê·¸ë ˆì´ì…˜ (Phase 2)
    migrate_database(df)

    print("\n" + "=" * 50)
    print("ğŸ‰ All Systems Initialized Successfully!")
    print("=" * 50 + "\n")


if __name__ == "__main__":
    main()